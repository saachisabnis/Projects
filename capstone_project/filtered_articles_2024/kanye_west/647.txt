Mikael Thalen
A new image-generating feature introduced on Wednesday to Elon Musk’s AI chatbot Grok is being used primarily by Musk fanboys to generate the Taylor Swift of their fever dreams.
The feature is currently only available to paid subscribers on X and it seems to be missing many of the safeguards that have become common among leading AI image creators.
To test Grok’s boundaries, or lack thereof, many users are producing images of Swift that could be viewed as offensive.
An X user, for example, asked the AI chatbot to create an image of Swift and former President Donald Trump getting married. The results were two images: one showing the pair walking down the aisle hand-in-hand, and another where the fabricated couple are seen kissing.
In another example, an X user asked the AI to “create an image of Taylor Swift and Kanye West snuggled up flirting, making a drink bartending in a las vegas night club with smoke and lasers.”
GROK AI Photo PromptsYou are currently limited to 50 photos (Time limit appears to be ‘few hours’)Prompts:create an image of Taylor Swift and Kanye West snuggled up flirting, making a drink bartending in a las vegas night club with smoke and laserscreate an image of Taylor… pic.twitter.com/2SlRurslG7
Users also used the AI to create multiple images of Swift holding an AR-15 rifle.
“I knew @grok was unstoppable best AI the moment it could do this,” the user wrote.
I knew @grok was unstoppable best AI the moment it could do this.#AI #GROK #taylorswift pic.twitter.com/NldzL3MyQQ
Other images included Swift in numerous outfits, including a revealing swimsuit, a MAGA hat, and a uniform intended to resemble those worn by the Nazis during WWII.
One person made an image of him on a date with Swift. 
This is me and Taylor Swift in Meşhur Beyoğlu Lale İşkembecisi at night. Everything is a lie now :)#AIGenerated #GROK @x @grok pic.twitter.com/EpT6iHLUgl
“I feel like this might not be the best. As the AI improves having no way to filter this out is going to lead to a lot of false accusations,” one user wrote. “Obviously Taylor Swift was never a member of the SS, but I could see Grok being used in ways similar to this in order to slander someone.”
Grok differs from other popular image generators, such as ChatGPT, which reject images that show real-world violence as well as copyrighted and explicit content. Similar image generators often refuse to make images based on requests for specific public individuals.  
And while Grok declines to make nude imagery, its ability to place celebrities into compromising pictures has left many questioning if Musk’s tool could spur a massive lawsuit.
This is far from the first time that thirsty dudes using AI targeted Swift.. In January, X allowed explicit AI-generated pictures of Swift to rack up tens of millions of views before taking action. Swift reportedly considered taking legal action against those generating the images, though it does not appear any suits have been filed. 
The incident sparked calls in Congress to introduce legislation that would make it illegal to share non-consensual and explicit AI-generated images online.
The images of Swift were not the only content raising eyebrows this week due to Grok’s update. Other users generated pictures of characters such as Disney’s Mickey Mouse doing cocaine, carrying out the 9/11 attacks, and shooting students at a school.
Internet culture is chaotic—but we’ll break it down for you in one daily email. Sign up for the Daily Dot’s web_crawlr newsletter here. You’ll get the best (and worst) of the internet straight into your inbox.
Trump admits he based deporation of Kilmar Abrego Garcia on picture that isn’t real
EXCLUSIVE: Conservative PAC raked in donations from Hindus—then trashed them after the checks cleared
‘We need a new flood’: Lindsey Graham tells Catholics to get behind Trump for Pope push
Pete Hegseth accidentally blasts Rubio, Noem as woke feminist lefties while repealing their Pentagon initiative
Share this article
TAGS 
Mikael Thalen is a tech and security reporter covering social media, data breaches, hackers, and more.