Why are we asking for donations?







                                            Why are we asking for donations?
                                        

                                            This site is free thanks to our community of supporters. Voluntary donations from readers like you keep our news accessible for everyone.
                                        
Donation Options


Search
Search
Search
Taylor Swift waves after the Kansas City Chiefs beat the Baltimore Ravens in the AFC Championship NFL football game Sunday. Swift was the victim of AI-generated pornographic images.
Nick Wass/AP
Share
I honestly hadn’t thought about the image in a very long time, mercifully. It’s been 12 years since it first hit the internet, and I’m happy to say I, and everyone else, have moved on.
But I was 33 years old when Hustler, the Larry Flynt smut rag, published what we’ve come to know now as a deepfake image of me … with a penis in my mouth.
The accompanying headline read, pointedly, “What Would S.E. Cupp Look Like with a Dick in Her Mouth?”
I was apparently targeted by Flynt for the sin of being a conservative woman. “Her hotness is diminished,” the piece read, “when she espouses dumb ideas like defunding Planned Parenthood.”
I was on set at a TV network when a friend called to tell me about the very graphic image of me that was now circulating. What happened immediately was a mix of panic and nausea. How many people are going to see this? How many are going to think it’s real?
Then came the awful task of telling my colleagues and bosses the humiliating story. I had to call my parents and family members. I called other employers I worked for. All were sympathetic, of course. But that didn’t make it any less horrifying. I had protected my reputation doggedly, and now I was having to tell people I respected that I was somehow in Hustler.
I got a ton of support, not only from my employers, but from the women at “The View,” as well as Gloria Steinem, NARAL and Planned Parenthood, to their great credit.
But despite the support, I actually felt worse as time went on. I felt dirty and ashamed, as if I’d actually done the thing I was shown doing. I was sick at the thought of anyone seeing me that way, and worried that one day my future children would see that somewhere in the bowels of the internet.
Back then, it was defended as an issue of free speech and satire. Flynt, who had won many a lawsuit over these sorts of things, was defiant, saying, “As the result of our publishing an ad parody of political pundit S.E. Cupp that depicted her having oral sex, the prudish and delusional right wing has accused me and my magazine of being sexist and waging a war on women. That’s absurd.”
Of course, that’s exactly what it was, and it was intended to degrade and shame me. He went even further, saying, “Find another horse to beat. We don’t know anything about Ms. Cupp’s personal life, but we do know that oral sex is practiced by the majority of adult Americans, both male and female.”
In other words, the image we manufactured is probably not all that fake.
This awful chapter came rushing back to me with deepfakes of Taylor Swift, generated by artificial intelligence, circulating on social media. They showed her in graphic, sexualized positions at a Kansas City Chiefs game.
While she benefits from being famous, so her fans presumably know they’re fake, many others perusing the web might not — or don’t care if they are. Swift hasn’t spoken about them yet, but I can only imagine how she’s feeling.
These images are a horrific violation of a woman’s body, integrity and privacy, and they can have a very deep impact on a woman’s psyche and self-worth.
That it’s happening to a massive celebrity like Swift is likely no consolation for the thousands of teen girls and young women it’s also happening to — girls and women who have no public recourse and perhaps no resources to fight them or prove to friends, family, and employers that they are not real.
Online bullying, including revenge porn and images like these, has led to a plethora of high-profile suicides among adolescent girls, and at least 13% of all adolescents have made serious suicide attempts because of cyberbullying.
Deepfake pornmakes up 96% of all deepfakes, almost exclusively targeting women.
U.S. Rep. Joe Morelle has authored the bipartisan Preventing Deepfakes of Intimate Images Act. “Deepfake pornography is sexual exploitation, it’s abusive, and I’m astounded it is not already a federal crime,” he said.
HR 3106 would prohibit the non-consensual disclosure of digitally altered intimate images, which makes the sharing of these images a criminal offense, and creates a right of private action for victims to seek damages.
If this had existed 12 years ago, I might not have had to endure the humiliating experience of being sexually exploited, shamed, and then told dismissively and callously to “find another horse to beat.”
I survived that experience, but not every woman will. It’s time to end this assault on women — pass HR 3106.
S.E. Cupp is the host of “S.E. Cupp Unfiltered” on CNN.
Send letters to letters@suntimes.com
Share
© 2025 Chicago Sun-Times Media, Inc.
Terms of Use • Privacy Notice • Cookie Policy • Terms of Sale