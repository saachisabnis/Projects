Telegram group used Microsoft tool to make explicit fake AI Taylor Swift images.
    
Fake AI images sexualizing Taylor Swift spread to X, formerly known as Twitter, from a Telegram group dedicated to sharing "abusive images of women," 404 Media reported.
These images began circulating online this week, quickly sparking mass outrage that may finally force a mainstream reckoning with harms caused by the spread of non-consensual deepfake pornography.
At least one member of the Telegram group claimed to be the source of some of the Swift images, posting in the channel that they didn't know if they "should feel flattered or upset that some of these Twitter stolen pics are my gen."
While it's still unknown how many AI tools were used to generate the flood of harmful images, 404 Media confirmed that some members of the Telegram group used Microsoft's free text-to-image AI generator, Designer.
According to 404 Media, the images were not created by training an AI model on Taylor Swift's images but by hacking tools like Designer to override safeguards designed to stop tools from generating images of celebrities. Members of the group shared strategies for subverting these safeguards by avoiding prompts using "Taylor Swift" and instead using keywords like "Taylor 'singer' Swift." They were then able to generate sexualized images by using keywords describing "objects, colors, and compositions that clearly look like sexual acts," rather than attempting to use sexual terms, 404 Media reported.
It's possible that Microsoft has already updated the tool to stop users from abusing Designer. 404 Media and Ars were not able to replicate outputs based on recommendations in the Telegram group. However, images of Swift can still be generated using the recommended keyword hack.
So far, Microsoft has not yet verified that the images were created using any of its AI tools, but the company is taking steps to strengthen filters on prompts to prevent future misuse in the meantime.
A Microsoft spokesperson told Ars that the tech giant is “investigating these reports" and has "taken appropriate action to prevent the misuse of our tools." The spokesperson also noted that Microsoft's Code of Conduct prohibits the use of Microsoft tools "for the creation of adult or non-consensual intimate content, and any repeated attempts to produce content that goes against our policies may result in loss of access to the service."
"We have teams working on the development of guardrails and other safety systems in line with our responsible AI principles, including content filtering, operational monitoring, and abuse detection to mitigate misuse of the system and help create a safer environment for users,” Microsoft's spokesperson said.
Some members of the Telegram channel appeared amused to see the images spread, not just on social media but also on sites featuring celebrity nudes and stolen adult content, 404 Media reported. But others scolded members for sharing the images outside the group and risking the channel being shut down.
Telegram has so far not responded to requests to comment.
Leaked from Telegram, a wide variety of fake images targeting Swift began spreading on X yesterday.
Ars found that some posts have been removed, while others remain online, as of this writing. One X post was viewed more than 45 million times over approximately 17 hours before it was removed, The Verge reported. Seemingly fueling more spread, X promoted these posts under the trending topic "Taylor Swift AI" in some regions, The Verge reported.
The Verge noted that since these images started spreading, "a deluge of new graphic fakes have since appeared." According to Fast Company, these harmful images were posted on X but soon spread to other platforms, including Reddit, Facebook, and Instagram. Some platforms, like X, ban the sharing of AI-generated images but seem to struggle with detecting banned content before it becomes widely viewed.
Ars' AI reporter Benj Edwards warned in 2022 that AI image-generation technology was rapidly advancing, making it easy to train an AI model on just a handful of photos before it could be used to create fake but convincing images of that person in infinite quantities. At first, it seemed like this might have been what happened to Swift, but 404 Media's report confirmed that the fake AI images were not classic “deepfakes" but took advantage of free tools trained on large quantities of public data.
It's currently unknown how many different non-consensual AI images of Swift have been generated or how widely those images have spread. 404 Media found "tens of thousands of bookmarks and likes and thousands of reposts" of the images online.
It's also unknown what consequences have resulted from the spread of the images. At least one verified X user had their account suspended after sharing fake images of Swift, The Verge reported, but Ars reviewed posts on X from Swift fans targeting others who allegedly shared images whose accounts remain active. Swift fans also have been uploading countless favorite photos of Swift to bury the harmful images and prevent them from appearing in various X searches. Her fans seem dedicated to reducing the spread however they can, with some posting different addresses, seemingly in attempts to dox an X user who they've alleged is the initial source of the images.
X has so far not commented directly on the images, but the X Safety account posted early Friday to remind users that "posting Non-Consensual Nudity (NCN) images is strictly prohibited on X and we have a zero-tolerance policy towards such content."
"Our teams are actively removing all identified images and taking appropriate actions against the accounts responsible for posting them," X Safety posted. "We're closely monitoring the situation to ensure that any further violations are immediately addressed, and the content is removed. We're committed to maintaining a safe and respectful environment for all users."
Swift's team has not yet commented on the deepfakes, but it seems clear that solving the problem will require more than just requesting removals from social media platforms. 404 Media's report shows how easy it is to take advantage of free tools to generate more harmful images, and it's also relatively easy to train an AI model on Swift's images using one of the known websites that specialize in making fine-tuned celebrity AI models. As long as these tools exist, anyone motivated to do so could crank out as many new images as they wanted, making it hard for even someone with Swift's resources to make the problem go away for good.
In that way, Swift's predicament might raise awareness of why creating and sharing non-consensual intimate imagery and deepfake pornography is harmful, perhaps moving the culture away from persistent notions that nobody is harmed by non-consensual AI-generated fakes.
Swift's plight could also inspire regulators to act faster to combat non-consensual deepfake porn. Last year, she inspired a Senate hearing after a Live Nation scandal frustrated her fans, triggering lawmakers' antitrust concerns about the leading ticket seller, The New York Times reported.
Some lawmakers are already working to combat deepfake porn. Congressman Joe Morelle (D-NY) proposed a law criminalizing deepfake porn earlier this year after teen boys at a New Jersey high school used AI image generators to create and share non-consensual fake nude images of female classmates. Under that proposed law, anyone sharing deepfake pornography without an individual's consent risks fines and being imprisoned for up to two years. Damages could go as high as $150,000 and imprisonment for as long as 10 years if sharing the images facilitates violence or impacts the proceedings of a government agency.
Elsewhere, the UK's Online Safety Act restricts any illegal content from being shared on platforms, including deepfake pornography. It requires moderation, or companies will risk fines worth more than $20 million, or 10 percent of their global annual turnover, whichever amount is higher.
The UK law, however, is controversial because it requires companies to scan private messages for illegal content. That makes it practically impossible for platforms to provide end-to-end encryption, which the American Civil Liberties Union has described as vital for user privacy and security.
As regulators tangle with legal questions and social media users with moral ones, some AI image generators have moved to limit models from producing NSFW outputs. Some did this by removing some of the large number of sexualized images in the models' training data, such as Stability AI, the company behind Stable Diffusion. Others, like Microsoft's Bing image creator, make it easy for users to report NSFW outputs. But 404 Media's report shows how easy it can be for users to develop creative keywords to skirt protections against NSFW outputs.
So far, keeping up with reports of deepfake porn seems to fall squarely on social media platforms' shoulders. Swift's battle this week shows how unprepared even the biggest platforms are to handle blitzes of harmful images seemingly uploaded faster than they can be removed.
This post was updated on January 26 to add comments from X Safety and new details from 404 Media.

    Listing image:
    
      Axelle/Bauer-Griffin / Contributor | FilmMagic
    

Ars Technica has been separating the signal from
          the noise for over 25 years. With our unique combination of
          technical savvy and wide-ranging interest in the technological arts
          and sciences, Ars is the trusted source in a sea of information. After
          all, you don’t need to know everything, only what’s important.