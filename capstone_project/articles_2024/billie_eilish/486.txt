Advertisement

                May 23, 2024 | 9 min read
            


                        Listen to article
                    
4 min

The company, which has quickly risen to the forefront of the AI industry, is no stranger to criticism and lawsuits from publishers and entertainers. But a flurry of controversies over the past week has eroded its public image even further. 
Many people have pointed out that one of the voices of GPT-4o sounds a lot like Scarlett Johansson’s. / Adobe Stock
For many of us, it’s been surreal to watch the pace at which AI has been advancing over the past couple of years. For actor Scarlett Johansson, the past 10 days have probably been particularly bizarre and disturbing.
On May 13, OpenAI – the AI developer that gained international fame in late 2022 with the launch of its generative AI platform ChatGPT – unveiled GPT-4o, a new model capable of responding to text, audio and visual cues. (The “o“ in the name stands for omni,” as in “omnidirectional” or “omnipotent.”) Crucially, the model “speaks“ in a spritely female voice that, according to Johansson and much of the internet, sounds a lot like her own.
In a statement following the release of GPT-4o’s first public demo, Johansson wrote that she was contacted twice by OpenAI CEO Sam Altman asking if she’d consider providing the voice for the model. In both cases, she declined the offer.
“When I heard the released demo, I was shocked, angered and in disbelief that Mr. Altman would pursue a voice that sounded so eerily similar to mine that my closest friends and news outlets could not tell the difference,” Johansson wrote in her statement.
Statement from Scarlett Johansson on the OpenAI situation. Wow: pic.twitter.com/8ibMeLfqP8
— Bobby Allyn (@BobbyAllyn) May 20, 2024
Theories about OpenAI’s intention were catapulted to a new level when Altman, on the same day of GPT-4o’s release, tweeted a single word: “her” – apparently a reference to the 2013 Spike Jonze film of the same name. The movie tells the story of a lonely man who falls in love with an AI assistant, the voice of which was played by Johansson.
OpenAI, however, has denied that that particular voice for GPT-4o, nicknamed Sky, was intentionally modeled on Johansson’s. (The model also comes with four other voice options.) Nonetheless, the company has bowed to the significant public blowback since the model’s first public demo. “Out of respect for Ms. Johansson, we have paused using Sky’s voice in our products,” Altman wrote in a statement on May 20. “We are sorry to Ms Johansson that we didn’t communicate better.”
The company added in a blog post that the voice of Sky was provided by “a different professional actress using her own natural speaking voice.” The Washington Post – citing, among other sources, documents shared by OpenAI – reported that the company had hired the voice actor for Sky months prior to the first time Altman reached out to Johansson, in September.
The public backlash following Johansson’s statement presents a serious issue for OpenAI, says Ray Hennessey, CEO of marketing firm Vocatus. “She is very popular and well-liked generally, and she has an audience that is naturally inclined to believe that she was wronged and defrauded,“ he says. “It isn’t hard to have a media narrative where the big, bad tech wolf came and victimized someone everyone likes.“ OpenAI, in Hennessey’s view, “needs to be aggressive in assuring the public that it isn’t here to steal the best of what humans have to offer, whether it’s their thinking, their intellectual property, their image, or, in the case of Johansson, their very voice. It should come out and issue a forceful apology and lay out steps to ensure that intellectual property is protected.“
The episode is just the latest in a long string of clashes between the AI firm and the entertainment and media industries.
A number of major news publishers, including The New York Times, have sued OpenAI and Microsoft (its lead investor), citing claims that copyrighted materials were used illegally for the purpose of training a large language model. Meanwhile, the unregulated use of AI in Hollywood was a central concern in the recent actors’ and writers’ strikes. Then, early last month, a number of prominent musicians – including Nicki Minaj, Katy Perry, Billie Eilish and Stevie Wonder – signed an open letter calling for better protections for musicians in the age of AI.
Through the course of the controversies, OpenAI and Altman have consistently maintained that they’re on the side of human creativity and that they have no intention of illegally or nonconsensually using other people’s or organizations' intellectual property.
The negative spotlight on OpenAI following the Johansson debacle has been made even more glaring by two other fresh controversies.
Ilya Sutskever and Jan Leike, both of whom formally oversaw the company’s superalignment team – responsible for ensuring that AI systems smarter than human beings will act in accordance with human interests – resigned from the company last week. Sutskever had also been one of the leading forces in the boardroom coup that briefly ousted Altman as CEO in November.
Following his resignation, Leike wrote on X that “over the past years, safety culture and processes have taken a backseat to shiny products” at OpenAI.
On May 18, Vox reported that OpenAI has strict non-disclosure and non-disparagement clauses in its off-boarding documents for former employees, prohibiting them from publicly criticizing the company – at the risk of losing all of their vested equity. Altman tweeted the following day that he was unaware of the clauses in question, that he was “embarrassed” by them, and that the company was in the process of changing them.
"We have not and never will take away vested equity, even when people didn't sign the departure documents," an OpenAI spokesperson told The Drum. "We'll remove nondisparagement clauses from our standard departure paperwork, and we're releasing former employees from existing nondisparagement obligations unless the nondisparagement provision was mutual. We're communicating this message to former employees. We're incredibly sorry that we're only changing this language now; it doesn't reflect our values or the company we want to be."
All of this has led Geoffrey Miller, a professor of evolutionary psychology at the University of New Mexico who also studies AI alignment, to compare Altman to another Sam. “The evidence for Sam Altman being a bad actor seems, [in my honest opinion], at least as compelling as the evidence for Sam Bankman-Fried being a bad actor before the FTX collapse in [November] 2022,” he wrote Tuesday in an online forum for the Effective Altruism movement. “And the stakes are much, much higher for humanity…”
Andrés Diana, chief innovation officer at software company Accrete AI, says that now would be a good time for OpenAI to be a bit more true to its name. The company “stands at a pivotal juncture where the path forward demands not just innovation but heightened transparency and ethical governance,” he tells The Drum.
The company should, for example, “be forthright about their data sources and training methodologies,“ Diana argues. “While there are undoubtedly competitive and legal complexities involved, willingness to disclose details about their data sources and training processes would not only clarify the company’s methodologies but also strengthen stakeholder trust, aligning with our industry’s push towards greater accountability.”
Not everyone believes that OpenAI is in dangerously hot water. “While firing Sam Altman was definitely a PR emergency, other recent happenings – like staff departures, minor controversies and updates to legal documents – are fairly standard for any tech company,“ says Ann Koppuzha, an attorney specializing in the e-commerce industry. “Companies like Uber, Meta and Google have faced far more tumultuous weeks. Remember the Cambridge Analytica scandal that brought Meta to the forefront of EU regulators? Now that was bad.“
For more on the latest happenings in AI, web3 and other cutting-edge technologies, sign up for The Emerging Tech Briefing newsletter.
Marketing can change the world.

        © Carnyx Group Ltd 2025 | The Drum is a Registered Trademark and property of Carnyx Group Limited. All rights reserved.