ByHamilton Mann
ByHamilton Mann,
 Contributor. 
Developing code of trust
As AI becomes increasingly embedded in our world, from mundane shopping decisions to critical medical diagnoses, and influences sectors as diverse as finance, transportation and justice, a new imperative emerges alongside its technological proliferation: the need for trust in AI.
Recent events in the news highlight this growing concern around trust in AI. A notable example is the controversy surrounding the creation of deepfake explicit images of Taylor Swift that spread on social media and sparked widespread outrage.
One might wonder, what does it truly mean to trust an AI system?
From a psychological and sociological perspective, as presented in the research study “Assessing Two Dimensions of Interpersonal Trust,” by Ming Zhang, published in Frontiers in Psychology, it is also understood as the willingness of an individual to be vulnerable to the actions of another party, based on the expectation that the other will perform a particular action important to the trustor, irrespective of the ability to monitor or control that other party.
This is a “lâcher-prise.” And what gives the order to let go and take the risk is based on the perception of what seems to us worthy or deserved whether we grant our trust or not.
These perceptions do not make a single reality, but several, such as in the case of deepfakes.
Trustworthy AI is just as much a non-monolithic concept, marked by diverse interpretations and, above all, whatever one does, a good dose of human subjectivity.
Some argue that it's not just about whether these systems can perform tasks accurately and efficiently or its reliability under different conditions; it's about how the AI algorithms function, in a safe manner, whether in its relationship to humans or more broadly, in its relationship to the environment. Take, for instance, the error made by Air Canada's customer service chatbot, which led a passenger to pay an extra $1,000 for his plane ticket, or the pedestrian accident involving a General Motors’ Cruise robottaxi leading to the recall of 950 vehicles.
Others highlight that it's not just about the relevance of outputs or outcomes; it's about understanding and having confidence in how the AI algorithms make decisions, how they come to certain conclusions, to which extend they accept to generate some particular results, how they interact with humans and how they affect society at large.
Additionally, others may emphasize that it's not just about the capabilities of AI to offer explicability and transparency, but also its adaptability to user needs for optimal interpretability. Consider an AI system designed to assist radiologists in diagnosing diseases from medical imaging. While the AI might have high accuracy and the capability to showcase and explain its diagnostic process (transparency and explicability), it also needs to adapt to the specific needs of different radiologists (interpretability). For instance, a senior radiologist might prefer the AI to provide detailed explanations about its diagnostic choices, highlighting particular features in the images and referencing relevant medical literature. On the other hand, less experienced radiologists might benefit more from a simplified overview and educational pointers that help them learn.
Further, the conversation should acknowledge that it's not only about addressing privacy and copyright concerns as exemplified by the New York Times' lawsuit against OpenAI; it's also about how AI actively enforces measures to mitigate biases in its algorithms to ensure fair treatment for all users and avoids becoming a conduit for the spread of disinformation and misinformation.
And still others might stress that trustworthy AI is not just about the way it can augment human intelligence while preserving human agency; it is about how their design and operation can uphold human rights and values.
There are other factors that are less commonly cited but also contribute to the creation of trust in AI, sometimes even more predominantly than the aforementioned criteria.
AI systems do not operate in a vacuum. They are integrated into the system of our lives and our societies. What influences the trust we place in them is not intrinsically limited to the AI system itself in isolation from the rest of the social ecosystem of which it is a part.
Trust in the AI system is therefore also created by factors such as reputation, not just that of the AI system as such, but also that of the company or organization that develops it. This trust is likely to grow due to the publicity it receives: word of mouth.
And more insidiously still, this trust is built through use, through habits, and more pervasively through the affordance it creates. Successful affordance in design allows users to intuitively understand how to interact with a system or product without needing explicit instructions. This intuitive understanding facilitates trust.
Due to the comfort that AI can bring, just like humans, technologies are capable of abusing trust.
In exchange for the ease of creating connections and social interactions, the advent of social media platforms has marked a significant change in our perceptions of privacy as we have gradually adapted to sharing more personal information online, developing a habit of trusting and often overlooking potential risks.
Finally, having trust in AI is actually something quite different from the concept of trust among humans, because the machine is not the equal of the human.
A Cambridge University Press study published in the journal Personality Neuroscience by provides insights into the trust people place in AI compared to that of their fellow humans. The research suggests that the mechanisms of forming trust with humans as opposed to machines with AI might involve different psychological and neural processes.
Actually, the matter of the trust we give, and our quest to extend this trust to AI, highlights the question of our growing reliance on AI. There are areas where the trust placed in AI exceeds that which we would be inclined to give to a human, as suggested in the research study “People may trust computers more than humans,” published by the University of Georgia.
As AI systems become more sophisticated, how far do we accept the risk of conditioning ourselves to decisions made by AI, much like we implicitly became conditioned to reconsider the boundaries of privacy since the advent of social networks?
Just as the convenience offered by social networks platforms led to our tacit acceptance of reduced privacy, there's a risk that the benefits provided by AI could lead to a gradual erosion of human agency.
This is a key societal question that will shape our future.
It is about what we define in dealing with the trade-off between two Cs: convenience and control.
For convenience, we are already accepting, and may increasingly implicitly accept, not being willing to understand, and thus not having control over, the reasoning behind many decisions made or influenced by AI. These could be observed in areas such as recruitment screening processes, legal writing in the justice system and loan assessments in banking, where AI systems are replacing some manual tasks for increased convenience.
In the AI era we live, we will have to do everything possible, including but not limited to erecting safeguards, so that taking the time and effort to understand does not become a human behavior in danger of extinction.